{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP first step, vectorization:  words to numbers\n",
    "\n",
    "We want to eventually train a machine learning algorithm to take in a headline and tell us how many upvotes it would receive. However, machine learning algorithms only understand numbers, not words. How do we translate our headlines into something an algorithm can understand?\n",
    "\n",
    "The first step is to create something called a **bag of words** matrix. A bag of word matrix gives us a numerical representation of which words are in which headlines.\n",
    "\n",
    "In order to construct a bag of words matrix, we first find the unique words across the whole set of headlines. Then, we setup a matrix where each row is a headline, and each column is one of the unique words. Then, we fill in each cell with the number of times that word occured in that headline.\n",
    "\n",
    "This will result in a matrix where a lot of the cells have a value of zero, unless the vocabulary is mostly shared between the headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas\n",
    "\n",
    "text = [\n",
    "    \"Programming language Python is simple programming language\",\n",
    "    \"Machine Learning with Python is simple\",\n",
    "    \"Machine Learning with Java is common\",\n",
    "    \"Java is Object Oriented Programming language\",\n",
    "    \"Machine Learning with R is old\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   oriented  java  language  python  is  object  programming  machine  simple  \\\n",
      "0         0     0         2       1   1       0            2        0       1   \n",
      "1         0     0         0       1   1       0            0        1       1   \n",
      "2         0     1         0       0   1       0            0        1       0   \n",
      "3         1     1         1       0   1       1            1        0       0   \n",
      "4         0     0         0       0   1       0            0        1       0   \n",
      "\n",
      "   r  common  learning  old  with  \n",
      "0  0       0         0    0     0  \n",
      "1  0       0         1    0     1  \n",
      "2  0       1         1    0     1  \n",
      "3  0       0         0    0     0  \n",
      "4  1       0         1    1     1  \n"
     ]
    }
   ],
   "source": [
    "# Find all the unique words in the text.\n",
    "unique_words = list(set(\" \".join(text).lower().split()))\n",
    "def make_matrix(text, vocab):\n",
    "    matrix = []\n",
    "    for sentence in text:\n",
    "        s = sentence.lower().split()\n",
    "        # Count each word in the text, and make a dictionary.\n",
    "        counter = Counter(s)\n",
    "        # Turn the dictionary into a matrix row using the vocab.\n",
    "        row = [counter.get(w, 0) for w in vocab]\n",
    "        matrix.append(row)\n",
    "    df = pandas.DataFrame(matrix)\n",
    "    df.columns = unique_words\n",
    "    return df\n",
    "\n",
    "print(make_matrix(text, unique_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords\n",
    "Certain words don’t help you discriminate between good and bad headlines. Words such as the, a, and also occur commonly enough in all contexts that they don’t really tell us much about whether something is good or not. They are generally equally likely to appear in both good and bad headlines.\n",
    "\n",
    "By removing these, we can reduce the size of the matrix, and make training an algorithm faster.\n",
    "\n",
    "```python\n",
    "with open(\"stopwords_en.txt\", 'r') as f:\n",
    "    stopwords = f.read().split(\"\\n\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   oriented  java  language  python  object  programming  machine  simple  \\\n",
      "0         0     0         2       1       0            2        0       1   \n",
      "1         0     0         0       1       0            0        1       1   \n",
      "2         0     1         0       0       0            0        1       0   \n",
      "3         1     1         1       0       1            1        0       0   \n",
      "4         0     0         0       0       0            0        1       0   \n",
      "\n",
      "   common  learning  \n",
      "0       0         0  \n",
      "1       0         1  \n",
      "2       1         1  \n",
      "3       0         0  \n",
      "4       0         1  \n"
     ]
    }
   ],
   "source": [
    "# Find all the unique words in the text.\n",
    "unique_words = list(set(\" \".join(text).lower().split()))\n",
    "\n",
    "# remove stopwords\n",
    "with open(\"stopwords_en.txt\", 'r') as f:\n",
    "    stopwords = f.read().split(\"\\n\")\n",
    "\n",
    "unique_words = [w for w in unique_words if w not in stopwords]\n",
    "\n",
    "def make_matrix(text, vocab):\n",
    "    matrix = []\n",
    "    for sentence in text:\n",
    "        s = sentence.lower().split()\n",
    "        # Count each word in the text, and make a dictionary.\n",
    "        counter = Counter(s)\n",
    "        # Turn the dictionary into a matrix row using the vocab.\n",
    "        row = [counter.get(w, 0) for w in vocab]\n",
    "        matrix.append(row)\n",
    "    df = pandas.DataFrame(matrix)\n",
    "    df.columns = unique_words\n",
    "    return df\n",
    "\n",
    "print(make_matrix(text, unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a matrix \n",
    "\n",
    "Now that we know the basics, we can make a bag of words matrix for the whole set of headlines.\n",
    "\n",
    "We don’t want to have to code everything out manually every time, so we’ll use a class from scikit-learn to do it automatically. Using the vectorizers from scikit-learn to construct your bag of words matrices will make the process much easier and faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix\n",
      "(5, 11)\n",
      "Terms/Vocabularies\n",
      "[u'common', u'java', u'language', u'learning', u'machine', u'object', u'old', u'oriented', u'programming', u'python', u'simple']\n",
      "Matrix of text\n",
      "[[0 0 2 0 0 0 0 0 2 1 1]\n",
      " [0 0 0 1 1 0 0 0 0 1 1]\n",
      " [1 1 0 1 1 0 0 0 0 0 0]\n",
      " [0 1 1 0 0 1 0 1 1 0 0]\n",
      " [0 0 0 1 1 0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Construct a bag of words matrix.\n",
    "# This will lowercase everything, and ignore all punctuation by default.\n",
    "# It will also remove stop words.\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words=\"english\")\n",
    "\n",
    "matrix = vectorizer.fit_transform(text)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "# We created our bag of words matrix with far fewer commands.\n",
    "print \"Shape of matrix\"\n",
    "print(matrix.shape)\n",
    "\n",
    "print \"Terms/Vocabularies\"\n",
    "print vocab\n",
    "\n",
    "print \"Matrix of text\"\n",
    "print(matrix.todense())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab).index(u'language')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.68,  1.  ,  0.43,  1.  ],\n",
       "       [ 0.68,  0.  ,  0.5 ,  1.  ,  0.42],\n",
       "       [ 1.  ,  0.5 ,  0.  ,  0.78,  0.42],\n",
       "       [ 0.43,  1.  ,  0.78,  0.  ,  1.  ],\n",
       "       [ 1.  ,  0.42,  0.42,  1.  ,  0.  ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "dist = 1 - cosine_similarity(matrix)\n",
    "np.round(dist, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist[1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
